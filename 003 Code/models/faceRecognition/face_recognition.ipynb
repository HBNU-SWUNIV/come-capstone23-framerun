{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef0904b",
   "metadata": {},
   "source": [
    "# pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ce17e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install albumentations\n",
    "# !pip install torchsummary\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8bd434",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e096cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from retinaface import RetinaFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e4527f",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "730c561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 99\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()  \n",
    "device = torch.device(\"cuda\" if usae_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {} \n",
    "print(\"set vars and device done\")\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "282ede4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    env = 'default'\n",
    "    classify = 'softmax'\n",
    "    num_classes = 400 \n",
    "    metric = 'arc_margin'\n",
    "    easy_margin = False\n",
    "    use_se = False\n",
    "    loss = 'arcface_loss'\n",
    "\n",
    "    display = False\n",
    "    finetune = False # True \n",
    "\n",
    "    train_root = 'data/face_train'\n",
    "    valid_root = 'data/face_valid'\n",
    "    valid_list = 'data/pair.txt'\n",
    "    \n",
    "    checkpoints_path = 'checkpoints'\n",
    "    load_model_path = 'models/pretrain_model.pt'\n",
    "    test_model_path = 'set_your_pt.pt' \n",
    "    save_interval = 10 \n",
    "\n",
    "    train_batch_size = 32\n",
    "    test_batch_size = 10\n",
    "\n",
    "    input_shape = (3, 112, 112)\n",
    "\n",
    "    optimizer = 'adam' #'sgd'\n",
    "\n",
    "    use_gpu = True \n",
    "    gpu_id = '0, 1, 2, 3'\n",
    "    num_workers = 4  \n",
    "    print_freq = 100\n",
    "    max_epoch = 100\n",
    "    lr = 1e-1 # 1e-4\n",
    "    lr_step = 50\n",
    "    lr_decay = 0.95\n",
    "    weight_decay = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7771c536-d9c8-4ddf-b68e-ccac0a852344",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee6fc5",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cdd2253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_image_path = sorted(glob.glob(f'{opt.train_root}/*/*.jpg'))\n",
    "valid_image_path = sorted(glob.glob(f'{opt.valid_root}/**/*.jpg'))\n",
    "\n",
    "image = cv2.imread(train_image_path[0], cv2.IMREAD_COLOR)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4633b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, image_path, augmentation=None):\n",
    "        self.image_path = image_path\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        label = int(self.image_path[idx].split('/')[3]) # set a personal id\n",
    "            \n",
    "        image = cv2.imread(self.image_path[idx], cv2.IMREAD_COLOR)        \n",
    "        \n",
    "        if self.augmentation:\n",
    "            sample = {'image':image}\n",
    "            sample = self.augmentation(**sample)\n",
    "            image = sample['image']\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3b67fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "                 A.HorizontalFlip(p=0.7),\n",
    "                 A.Resize(112,112),\n",
    "                 #A.OneOf([\n",
    "                  #   A.HorizontalFlip(p=0.7),\n",
    "                   #  A.ShiftScaleRotate(p=0.5)\n",
    "                 #], p=1),\n",
    "                 #A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "                 #A.Normalize((0.643025, 0.49486318, 0.424703), (0.20040053, 0.17854902, 0.16714329)),\n",
    "                 ToTensorV2()\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "                 A.Resize(112,112),\n",
    "                #A.Normalize((0.643025, 0.49486318, 0.424703), (0.20040053, 0.17854902, 0.16714329)),\n",
    "                ToTensorV2() \n",
    "])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "47700559",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataLoader(train_image_path, augmentation = train_transform)\n",
    "valid_dataset = DataLoader(valid_image_path, augmentation = valid_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, opt.train_batch_size, shuffle=True, drop_last = True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, opt.test_batch_size , shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cb83a8",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9dcfbe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, eps=1e-7): \n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()\n",
    "    \n",
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.3): # s=30, m=0.5\n",
    "        super().__init__()\n",
    "        self.margin = ArcMarginProduct(in_features, out_features, s=s, m=m)\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        cosine = self.margin(x, labels)\n",
    "        logits = cosine / self.margin.s\n",
    "        loss = self.cross_entropy(logits, labels)\n",
    "        return loss\n",
    "    \n",
    "class ArcFocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, eps=1e-7, s=30.0):\n",
    "        super(ArcFocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.s = s\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        # Arcface loss\n",
    "        cos_theta = logits\n",
    "        sin_theta = torch.sqrt(1 - cos_theta ** 2)\n",
    "        phi_theta = cos_theta * self.s\n",
    "        log_probs = F.log_softmax(phi_theta, dim=-1)\n",
    "        one_hot_labels = F.one_hot(labels, num_classes=logits.size(-1))\n",
    "        arc_loss = -one_hot_labels * log_probs\n",
    "        arc_loss = arc_loss.sum(dim=-1)\n",
    "\n",
    "        # Focal loss\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        pt = probs * one_hot_labels + (1 - probs) * (1 - one_hot_labels)\n",
    "        w = (1 - pt).pow(self.gamma)\n",
    "        focal_loss = -w * torch.log(pt + self.eps)\n",
    "        focal_loss = focal_loss.sum(dim=-1)\n",
    "        loss = arc_loss * focal_loss\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9584c7",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bb0fe14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.3, easy_margin=False): \n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        input = input.to(device)\n",
    "        self.weight = nn.Parameter(torch.randn(512,512).cuda())\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
    "        output *= self.s\n",
    "        # print(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class AddMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin cosine distance: :\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        s: norm of input feature\n",
    "        m: margin\n",
    "        cos(theta) - m\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.40):\n",
    "        super(AddMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        input = input.to(device)\n",
    "        self.weight = nn.Parameter(torch.randn(512,512).cuda())\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        phi = cosine - self.m\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
    "        output *= self.s\n",
    "        # print(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "               + 'in_features=' + str(self.in_features) \\\n",
    "               + ', out_features=' + str(self.out_features) \\\n",
    "               + ', s=' + str(self.s) \\\n",
    "               + ', m=' + str(self.m) + ')'\n",
    "\n",
    "\n",
    "class SphereProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin cosine distance: :\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        m: margin\n",
    "        cos(m*theta)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, m=4):\n",
    "        super(SphereProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.m = m\n",
    "        self.base = 1000.0\n",
    "        self.gamma = 0.12\n",
    "        self.power = 1\n",
    "        self.LambdaMin = 5.0\n",
    "        self.iter = 0\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform(self.weight)\n",
    "\n",
    "        # duplication formula\n",
    "        self.mlambda = [\n",
    "            lambda x: x ** 0,\n",
    "            lambda x: x ** 1,\n",
    "            lambda x: 2 * x ** 2 - 1,\n",
    "            lambda x: 4 * x ** 3 - 3 * x,\n",
    "            lambda x: 8 * x ** 4 - 8 * x ** 2 + 1,\n",
    "            lambda x: 16 * x ** 5 - 20 * x ** 3 + 5 * x\n",
    "        ]\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # lambda = max(lambda_min,base*(1+gamma*iteration)^(-power))\n",
    "        self.iter += 1\n",
    "        self.lamb = max(self.LambdaMin, self.base * (1 + self.gamma * self.iter) ** (-1 * self.power))\n",
    "\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cos_theta = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        cos_theta = cos_theta.clamp(-1, 1)\n",
    "        cos_m_theta = self.mlambda[self.m](cos_theta)\n",
    "        theta = cos_theta.data.acos()\n",
    "        k = (self.m * theta / 3.14159265).floor()\n",
    "        phi_theta = ((-1.0) ** k) * cos_m_theta - 2 * k\n",
    "        NormOfFeature = torch.norm(input, 2, 1)\n",
    "\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        one_hot = torch.zeros(cos_theta.size())\n",
    "        one_hot = one_hot.cuda() if cos_theta.is_cuda else one_hot\n",
    "        one_hot.scatter_(1, label.view(-1, 1), 1)\n",
    "\n",
    "        # --------------------------- Calculate output ---------------------------\n",
    "        output = (one_hot * (phi_theta - cos_theta) / (1 + self.lamb)) + cos_theta\n",
    "        output *= NormOfFeature.view(-1, 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "               + 'in_features=' + str(self.in_features) \\\n",
    "               + ', out_features=' + str(self.out_features) \\\n",
    "               + ', m=' + str(self.m) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c4cf4",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972bf980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch, gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "def save_model(model, save_path, name, iter_cnt):\n",
    "    save_name = os.path.join(save_path, name + '_' + str(iter_cnt) + '.pt')\n",
    "    torch.save(model.state_dict(), save_name)\n",
    "    return save_name\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Set a loss function\n",
    "    if opt.loss == 'focal_loss':\n",
    "        criterion = FocalLoss(gamma=2)\n",
    "    elif opt.loss == 'arcface_loss':\n",
    "          criterion = ArcFaceLoss(512, 400)\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Set a metric\n",
    "    if opt.metric == 'add_margin':\n",
    "        metric_fc = AddMarginProduct(512, opt.num_classes, s=30, m=0.35)\n",
    "    elif opt.metric == 'arc_margin':\n",
    "        metric_fc = ArcMarginProduct(512, opt.num_classes, s=30, m=0.5, easy_margin=opt.easy_margin)\n",
    "    elif opt.metric == 'sphere':\n",
    "        metric_fc = SphereProduct(512, opt.num_classes, m=4)\n",
    "    else:\n",
    "        metric_fc = nn.Linear(512, opt.num_classes)\n",
    "\n",
    "    # Use a pretrain model's weight\n",
    "    ckpt = torch.load(opt.load_model_path, map_location=device)  # load checkpoints\n",
    "    model = ckpt['backbone'].to(device)\n",
    "    \n",
    "    # Set an optimizer\n",
    "    if opt.optimizer == 'sgd':\n",
    "        optimizer = torch.optim.SGD([{'params': model.parameters()}, {'params': metric_fc.parameters()}],\n",
    "                                    lr=opt.lr, weight_decay=opt.weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam([{'params': model.parameters()}, {'params': metric_fc.parameters()}],\n",
    "                                     lr=opt.lr, weight_decay=opt.weight_decay)\n",
    "    scheduler = StepLR(optimizer, step_size=opt.lr_step, gamma=0.1)\n",
    "\n",
    "    # Start training\n",
    "    print('{} train iters per epoch:'.format(len(train_loader)))\n",
    "    \n",
    "    start = time.time()\n",
    "    for i in range(opt.max_epoch):\n",
    "        model.train()\n",
    "        for ii, (data, label) in enumerate(train_loader):\n",
    "            data, label = data.type(torch.float32).to(device), torch.tensor(list(label)).to(device)\n",
    "            \n",
    "            # Get a feature embedding\n",
    "            feature = model(data)\n",
    "            \n",
    "            # Compute a similarity\n",
    "            output = metric_fc(feature, label)\n",
    "            \n",
    "            loss = criterion(output, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            iters = i * len(train_loader) + ii\n",
    "\n",
    "            if iters % opt.print_freq == 0:\n",
    "                output = output.data.cpu().numpy()\n",
    "                output = np.argmax(output, axis=1)\n",
    "                label = label.data.cpu().numpy()\n",
    "                \n",
    "                # print(output, label)\n",
    "                    \n",
    "                acc = np.mean((output == label).astype(int))\n",
    "                speed = opt.print_freq / (time.time() - start)\n",
    "                time_str = time.asctime(time.localtime(time.time()))\n",
    "                print('epoch {} | {} iter {} {} iters/s | loss {} | acc {}'.format(time_str, i, ii, speed, loss.item(), acc))\n",
    "\n",
    "                start = time.time()\n",
    "    \n",
    "        scheduler.step()\n",
    "        \n",
    "        if i % opt.save_interval == 0 or i == opt.max_epoch:\n",
    "            save_model(model, opt.checkpoints_path, opt.backbone, i)\n",
    "\n",
    "        model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef5ce0e-8dfa-4b59-aca0-4331fdc25b76",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d39bf40f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Return a list that stores only the unique identifiers of the comparison subjects\n",
    "def get_face_path(pair_list):\n",
    "    with open(pair_list, 'r') as fd:\n",
    "        pairs = fd.readlines()\n",
    "    img_path = []\n",
    "    for pair in pairs:\n",
    "        splits = pair.split()\n",
    "        \n",
    "        if splits[0] not in img_path:\n",
    "            img_path.append(splits[0])\n",
    "            \n",
    "        if splits[1] not in img_path:\n",
    "            img_path.append(splits[1])\n",
    "            \n",
    "    return img_path\n",
    "\n",
    "# Return a list that stores only the unique identifiers of the comparison subjects\n",
    "def get_face_list(pair_list):\n",
    "    with open(pair_list, 'r') as fd:\n",
    "        pairs = fd.readlines()\n",
    "    data_list = []\n",
    "    for pair in pairs:\n",
    "        splits = pair.split()\n",
    "\n",
    "        if splits[0].split('/')[3] not in data_list:\n",
    "            data_list.append(splits[0].split('/')[3])\n",
    "\n",
    "        if splits[1].split('/')[3] not in data_list:\n",
    "            data_list.append(splits[1].split('/')[3])\n",
    "            \n",
    "    return data_list\n",
    "\n",
    "def load_image(img_path):\n",
    "    image = cv2.imread(img_path)\n",
    "    \n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    image = cv2.resize(image, (112,112)) # Adjust the input size for the model\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    image = image[np.newaxis, :, :, :]\n",
    "    image = image.astype(np.float32, copy=False)\n",
    "    image -= 127.5 \n",
    "    image /= 127.5\n",
    "    \n",
    "    return image\n",
    "\n",
    "def get_features(model, img_paths, batch_size=10):\n",
    "    images = None\n",
    "    features = None\n",
    "    cnt = 0\n",
    "    for i, path in enumerate(img_paths):\n",
    "        image = load_image(path)\n",
    "        if image is None:\n",
    "            print('read {} error'.format(path))\n",
    "\n",
    "        if images is None:\n",
    "            images = image\n",
    "        else:\n",
    "            images = np.concatenate((images, image), axis=0)\n",
    "\n",
    "        if images.shape[0] % batch_size == 0 or i == len(img_paths) - 1:\n",
    "            cnt += 1\n",
    "            \n",
    "            data = torch.from_numpy(images)\n",
    "            data = data.to(torch.device(\"cuda\"))\n",
    "            output = model(data)\n",
    "            output = output.data.cpu().numpy()\n",
    "\n",
    "            feature1 = output[::2]\n",
    "            feature2 = output[1::2]\n",
    "            feature = np.hstack((feature1, feature2))\n",
    "\n",
    "            if features is None:\n",
    "                features = feature\n",
    "            else:\n",
    "                features = np.vstack((features, feature))\n",
    "\n",
    "            images = None\n",
    "  \n",
    "    return features, cnt\n",
    "\n",
    "# Return a dictionary in the format {individual unique identifier: individual embedding vector}\n",
    "def get_feature_dict(test_list, features): \n",
    "    fe_dict = {}\n",
    "    for i, each in enumerate(test_list):\n",
    "        fe_dict[each] = features[i]\n",
    "    return fe_dict\n",
    "\n",
    "# Measure similarity between two images and return accuracy\n",
    "def test_performance(fe_dict, pair_list):\n",
    "    with open(pair_list, 'r') as fd:\n",
    "        pairs = fd.readlines()\n",
    "\n",
    "    sims = []\n",
    "    labels = []\n",
    "    \n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    \n",
    "    same_cnt = 0\n",
    "    same_th = 0\n",
    "    diff_cnt = 0\n",
    "    diff_th = 0\n",
    "    for pair in pairs:\n",
    "        splits = pair.split()\n",
    "        feature1 = fe_dict[splits[0].split('/')[3]]\n",
    "        feature2 = fe_dict[splits[1].split('/')[3]]\n",
    "        label = int(splits[2])\n",
    "        \n",
    "        sim = cosin_similarity(feature1, feature2)\n",
    "        \n",
    "        if sim > 0.89 and label:\n",
    "            right += 1            \n",
    "        if sim > 0.89 and not label:\n",
    "            wrong += 1\n",
    "        if not label and sim < 0.89:\n",
    "            right += 1\n",
    "            \n",
    "        if label == 0:\n",
    "            diff_cnt +=1\n",
    "            diff_th += sim\n",
    "            \n",
    "        if label == 1:\n",
    "            same_cnt +=1\n",
    "            same_th += sim\n",
    "            \n",
    "        sims.append(sim)\n",
    "        labels.append(label)\n",
    "        \n",
    "    acc = cal_accuracy(sims, labels)\n",
    "    \n",
    "    print(\"valid acc: \",right / (right + wrong) * 100)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def cal_accuracy(y_score, y_true):\n",
    "    y_score = np.asarray(y_score)\n",
    "    y_true = np.asarray(y_true)\n",
    "    best_acc = 0\n",
    "    for i in range(len(y_score)):\n",
    "        th = y_score[i]\n",
    "        y_test = (y_score >= th)\n",
    "        acc = np.mean((y_test == y_true).astype(int))\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "def cosin_similarity(x1, x2):\n",
    "    return np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2)\n",
    "\n",
    "def test(model, img_paths, identity_list, compair_list, batch_size):\n",
    "    s = time.time()\n",
    "    features, cnt = get_features(model, img_paths, batch_size=batch_size)\n",
    "    print(features.shape)\n",
    "    t = time.time() - s\n",
    "    print('total time is {}, average time is {}'.format(t, t / cnt))\n",
    "    fe_dict = get_feature_dict(identity_list, features) \n",
    "    acc= test_performance(fe_dict, compair_list)\n",
    "    return acc\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    opt = Config()\n",
    "    \n",
    "    ckpt = torch.load(opt.test_model_path, map_location=device)  \n",
    "    model = ckpt['backbone'].to(device)\n",
    "    model = DataParallel(model)\n",
    "\n",
    "    identity_list = get_face_list(opt.valid_list) \n",
    "    img_paths = get_face_path(opt.valid_list)\n",
    "\n",
    "    model.eval()\n",
    "    test(model, img_paths, identity_list, opt.valid_list, opt.test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe886db-899b-48fb-bd90-a69794368549",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Get the someones' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a02bd94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(model, image, batch_size=1):\n",
    "    image = cv2.resize(image, (112,112))\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    image = image[np.newaxis, :, :, :]\n",
    "    image = image.astype(np.float32, copy=False)\n",
    "    image -= 127.5 \n",
    "    image /= 127.5\n",
    "    \n",
    "    data = torch.from_numpy(image)\n",
    "    data = data.to(torch.device(\"cuda\"))\n",
    "    feature = model(data)\n",
    "    feature = feature.data.cpu().numpy()[0]\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "978a562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(test_model_path, map_location=device)\n",
    "model = ckpt['backbone'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e237ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"someone.jpg\")\n",
    "output = get_feature(model, image, batch_size=1)\n",
    "# print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
